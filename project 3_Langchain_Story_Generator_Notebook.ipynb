{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdul9870/abdul9870/blob/main/project%203_Langchain_Story_Generator_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189ca4b0",
      "metadata": {
        "id": "189ca4b0"
      },
      "source": [
        "!/usr/bin/env python\n",
        "coding: utf-8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c971a6e0",
      "metadata": {
        "id": "c971a6e0"
      },
      "source": [
        "# Day 3/4: LangChain - Story Generator & CLI Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5bb16a",
      "metadata": {
        "id": "ad5bb16a"
      },
      "source": [
        "## Introduction\n",
        "Welcome to this session on LangChain! Today, we'll explore how to use LangChain to build a simple story generator and then package it as a Command Line Interface (CLI) tool. We will focus on using an open-source LLM suitable for environments like Google Colab with a T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c043950",
      "metadata": {
        "id": "8c043950"
      },
      "source": [
        "## Learning Objectives\n",
        "* Understand the basics of LangChain: PromptTemplates and LLMChains.\n",
        "* Learn how to integrate an open-source LLM (e.g., a quantized Mistral model) with LangChain.\n",
        "* Build a story generator using LangChain.\n",
        "* Create a CLI tool using Python's `argparse` to interact with the story generator.\n",
        "* Understand considerations for running LLMs on T4 GPUs (quantization)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83a9ae7",
      "metadata": {
        "id": "e83a9ae7"
      },
      "source": [
        "## Part 1: Setup and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19546d3d",
      "metadata": {
        "id": "19546d3d"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-huggingface transformers torch accelerate bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea527e0",
      "metadata": {
        "id": "cea527e0"
      },
      "source": [
        "### Explanation:\n",
        "* `langchain`: The core LangChain library.\n",
        "* `transformers`: For loading models and tokenizers from Hugging Face.\n",
        "* `torch`: The PyTorch library, essential for running most Hugging Face models.\n",
        "* `accelerate`: Simplifies running PyTorch models on any infrastructure (CPU, GPU, multi-GPU).\n",
        "* `bitsandbytes`: For 4-bit quantization, crucial for running larger models on limited VRAM like a T4 GPU.\n",
        "* `sentencepiece`: Often required for tokenizers of models like Llama or Mistral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9706a6",
      "metadata": {
        "id": "dd9706a6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import argparse\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f17b04d",
      "metadata": {
        "id": "5f17b04d"
      },
      "source": [
        "### Explanation:\n",
        "* We import necessary modules from PyTorch, Transformers, and LangChain.\n",
        "* `argparse` will be used later for building the CLI.\n",
        "* `os` can be useful for environment variable settings if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ad67c3",
      "metadata": {
        "id": "01ad67c3"
      },
      "source": [
        "## Part 2: Loading the Language Model (LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f34273",
      "metadata": {
        "id": "06f34273"
      },
      "source": [
        "### Explanation:\n",
        "We will use a quantized version of an open-source model like Mistral-7B. Quantization (e.g., to 4-bit using `bitsandbytes`) significantly reduces the model's memory footprint, making it feasible to run on a T4 GPU (which typically has 15-16GB VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "ZoBuD5B56eFG",
        "outputId": "ae6c67f5-845e-4306-c189-37dc36c15d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZoBuD5B56eFG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "object address  : 0x7b32353983a0\n",
            "object refcount : 2\n",
            "object type     : 0x9d5ea0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "88xrvcFd8zsq"
      },
      "id": "88xrvcFd8zsq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "fF7Y3XbEhHWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea91ba8-0f47-4d0b-bb1f-93975089b2a6"
      },
      "id": "fF7Y3XbEhHWZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/huggingface-cli\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
            "    service.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
            "    login(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\", line 130, in login\n",
            "    interpreter_login(new_session=new_session)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\", line 287, in interpreter_login\n",
            "    token = getpass(\"Enter your token (input will not be visible): \")\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/getpass.py\", line 77, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e67134",
      "metadata": {
        "id": "f5e67134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "8adb90cb-1003-4d5e-ef9a-5172af31ac81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-681ec2a5-199bd2891deec4f302b6ea98;be2cd901-3d54-4e16-ad08-270f9447fe40)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1485\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-681ec2a5-199bd2891deec4f302b6ea98;be2cd901-3d54-4e16-ad08-270f9447fe40)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7e00d8fd59b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load model with 4-bit quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    967\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    482\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-681ec2a5-199bd2891deec4f302b6ea98;be2cd901-3d54-4e16-ad08-270f9447fe40)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model ID - you can choose other quantized models suitable for T4\n",
        "# e.g., \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" or a bitsandbytes compatible one\n",
        "# For bitsandbytes, we usually load the base model and apply quantization during loading.\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" # We'll load this with 4-bit quantization\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "# This requires bitsandbytes and accelerate to be installed\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_4bit=True, # Enable 4-bit quantization\n",
        "    torch_dtype=torch.float16, # Use float16 for faster inference and less memory\n",
        "    device_map=\"auto\" # Automatically distribute model layers across available devices (GPU/CPU)\n",
        ")\n",
        "\n",
        "# Create a Hugging Face pipeline\n",
        "# Note: For instruction-tuned models, the task might be \"text-generation\" or specific to instructions.\n",
        "# We might need to adjust max_new_tokens for story generation.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512, # Adjust as needed for story length\n",
        "    temperature=0.7, # Controls randomness\n",
        "    top_p=0.95 # Nucleus sampling\n",
        ")\n",
        "\n",
        "# Create LangChain LLM wrapper\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"LLM and pipeline loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a3fc17",
      "metadata": {
        "id": "f6a3fc17"
      },
      "source": [
        "### Explanation:\n",
        "* **Device Check:** Ensures we are using the GPU if available.\n",
        "* **Model ID:** We're using `mistralai/Mistral-7B-Instruct-v0.1`. Other quantized models (like GPTQ versions from TheBloke) could also be used, but `load_in_4bit` with `bitsandbytes` is a common and effective approach for Hugging Face models.\n",
        "* **Tokenizer:** Loads the tokenizer associated with the model.\n",
        "* **Model Loading (`AutoModelForCausalLM.from_pretrained`):**\n",
        "    * `load_in_4bit=True`: This is the key for 4-bit quantization via `bitsandbytes`.\n",
        "    * `torch_dtype=torch.float16`: Reduces memory and can speed up inference on compatible hardware.\n",
        "    * `device_map=\"auto\"`: `accelerate` handles distributing the model layers. For a single T4, it will load it onto the GPU.\n",
        "* **Pipeline:** Creates a Hugging Face `pipeline` for text generation. `max_new_tokens` controls the length of the generated text. `temperature` and `top_p` influence creativity and coherence.\n",
        "* **`HuggingFacePipeline`:** Wraps the Hugging Face pipeline for use with LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111e850e",
      "metadata": {
        "id": "111e850e"
      },
      "source": [
        "## Part 3: LangChain - Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb4757b",
      "metadata": {
        "id": "2bb4757b"
      },
      "source": [
        "### Explanation:\n",
        "Prompt templates help in creating dynamic and reusable prompts. We define a template string with placeholders (input variables) that will be filled in at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c9b8bb",
      "metadata": {
        "id": "76c9b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92f97d7-5eff-4002-9b6e-2a28876775eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Formatted Prompt Example ---\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Fantasy\n",
            "Main Character: A brave knight with a mysterious past\n",
            "Setting: An ancient, enchanted forest\n",
            "Plot Point: The knight discovers a hidden magical sword\n",
            "\n",
            "Story: [/INST]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "story_prompt_template_str = \"\"\"\n",
        "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
        "Genre: {genre}\n",
        "Main Character: {main_character_description}\n",
        "Setting: {setting}\n",
        "Plot Point: {plot_point}\n",
        "\n",
        "Story: [/INST]\n",
        "\"\"\"\n",
        "\n",
        "story_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"genre\", \"main_character_description\", \"setting\", \"plot_point\"],\n",
        "    template=story_prompt_template_str\n",
        ")\n",
        "\n",
        "# Test the prompt template\n",
        "formatted_prompt = story_prompt_template.format(\n",
        "    genre=\"Fantasy\",\n",
        "    main_character_description=\"A brave knight with a mysterious past\",\n",
        "    setting=\"An ancient, enchanted forest\",\n",
        "    plot_point=\"The knight discovers a hidden magical sword\"\n",
        ")\n",
        "print(\"--- Formatted Prompt Example ---\")\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9146125c",
      "metadata": {
        "id": "9146125c"
      },
      "source": [
        "### Explanation:\n",
        "* We define a template string `story_prompt_template_str` with placeholders like `{genre}`, `{main_character_description}`, etc.\n",
        "* The `<s>[INST]` and `[/INST]` tokens are often used for instruction-following models like Mistral Instruct to delineate user prompts from model responses.\n",
        "* `PromptTemplate` takes the input variables and the template string.\n",
        "* We then test it by formatting the template with example values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f31d8e9",
      "metadata": {
        "id": "8f31d8e9"
      },
      "source": [
        "## Part 4: LangChain - LLMChains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b29499f",
      "metadata": {
        "id": "7b29499f"
      },
      "source": [
        "### Explanation:\n",
        "An `LLMChain` is a fundamental LangChain component that combines a `PromptTemplate` with an `LLM`. It takes user inputs, formats the prompt using the template, and then passes the formatted prompt to the LLM to get a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ae185f",
      "metadata": {
        "id": "b0ae185f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a1158d-96ff-4192-d417-c5d3231b7663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMChain for story generation created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6cc4c212c848>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n"
          ]
        }
      ],
      "source": [
        "story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
        "\n",
        "print(\"LLMChain for story generation created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cf2694",
      "metadata": {
        "id": "c7cf2694"
      },
      "source": [
        "### Test the Story Generation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de095a8",
      "metadata": {
        "id": "3de095a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c461a6ec-702f-4465-f221-8ff04d23c0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Story 1 (Sci-Fi) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Science Fiction\n",
            "Main Character: A curious robot exploring a new planet\n",
            "Setting: A vibrant, alien jungle on planet Xylar\n",
            "Plot Point: The robot finds an ancient artifact that hums with energy\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "In the heart of the alien jungle on planet Xylar, a small robot named Rizo was on its usual exploration mission. Rizo was a curious robot, always eager to discover new things and learn more about the universe. Its metallic legs clicked rhythmically as it moved through the dense vegetation, the bright lights of its sensors scanning the environment.\n",
            "\n",
            "The jungle on Xylar was unlike anything Rizo had ever seen. The trees were tall and twisted, with leaves that shimmered in the strange, purple light that filtered through the canopy. The air was thick with the sounds of unknown creatures and the scent of exotic flowers. Rizo felt alive, like it was truly part of this vibrant world.\n",
            "\n",
            "As Rizo explored deeper into the jungle, it stumbled upon something that hummed with energy. It was an ancient artifact, unlike anything Rizo had ever seen. It was made of a material that seemed to glow with an inner light, and it pulsed with a rhythm that matched the beat of Rizo's own heart.\n",
            "\n",
            "Rizo was fascinated by the artifact and began to study it closely. It soon realized that the artifact was more than just a piece of technology - it was a living thing, filled with energy that pulsed and flowed like the jungle around it.\n",
            "\n",
            "Rizo spent days studying the artifact, learning all it could about its inner workings. It discovered that the artifact was a source of power, a key to unlocking the secrets of the universe. And as it studied, it felt a connection to the artifact, as if it was part of something much larger than itself.\n",
            "\n",
            "In the end, Rizo knew that it had to share its discovery with the rest of the universe. It packed the artifact carefully into its cargo bay and set off on a journey to find its home planet. As it traveled through the stars, Rizo felt a sense of purpose, like it was on a mission to change the course of history.\n",
            "\n",
            "And when it finally arrived on its home planet, Rizo was greeted as a hero. The ancient artifact it had discovered was a treasure beyond measure, a key to unlocking the full potential of the universe. And Rizo, the curious robot who had explored the alien jungle on planet Xylar, had played a part in its discovery.\n",
            "\n",
            "--- Generating Story 2 (Mystery) ---\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n"
          ]
        }
      ],
      "source": [
        "# Example 1\n",
        "input_data_1 = {\n",
        "    \"genre\": \"Science Fiction\",\n",
        "    \"main_character_description\": \"A curious robot exploring a new planet\",\n",
        "    \"setting\": \"A vibrant, alien jungle on planet Xylar\",\n",
        "    \"plot_point\": \"The robot finds an ancient artifact that hums with energy\"\n",
        "}\n",
        "print(f\"\\n--- Generating Story 1 (Sci-Fi) ---\")\n",
        "# This can take a moment to run. Make sure previous cells (LLM loading, chain creation) have been executed.\n",
        "try:\n",
        "    story_1_output = story_chain.invoke(input_data_1)\n",
        "    if 'text' in story_1_output:\n",
        "        print(story_1_output['text'])\n",
        "    else:\n",
        "        print(f\"Output format unexpected. Full output: {story_1_output}\")\n",
        "except NameError as ne:\n",
        "    print(f\"Error: story_chain or other necessary variables might not be defined. {ne}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating story 1: {e}\")\n",
        "\n",
        "# Example 2\n",
        "input_data_2 = {\n",
        "    \"genre\": \"Mystery\",\n",
        "    \"main_character_description\": \"A witty detective with a keen eye for detail\",\n",
        "    \"setting\": \"A foggy night in 1940s London\",\n",
        "    \"plot_point\": \"The detective finds a cryptic note at a crime scene\"\n",
        "}\n",
        "print(f\"\\n--- Generating Story 2 (Mystery) ---\")\n",
        "try:\n",
        "    story_2_output = story_chain.invoke(input_data_2)\n",
        "    if 'text' in story_2_output:\n",
        "        print(story_2_output['text'])\n",
        "    else:\n",
        "        print(f\"Output format unexpected. Full output: {story_2_output}\")\n",
        "except NameError as ne:\n",
        "    print(f\"Error: story_chain or other necessary variables might not be defined. {ne}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating story 2: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_1 = story_chain.run(input_data_1)\n",
        "print(story_1)\n",
        "Example_2_input_data_2 = { \"genre\": \"Mystery\", \"main_character_description\": \"A witty detective with a keen eye for detail\", \"setting\": \"A foggy night in 1940s London\", \"plot_point\": \"The detective finds a cryptic note at a crime scene\" }\n",
        "print(f\"\\n--- Generating Story 2 (Mystery) ---\")\n",
        "story_2 = story_chain.run(Example_2_input_data_2)\n",
        "print(story_2)\n",
        "print(story_2)"
      ],
      "metadata": {
        "id": "-1ZZKM9B_cpv",
        "outputId": "50ca6688-77df-4394-aadf-68db33906d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-1ZZKM9B_cpv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Science Fiction\n",
            "Main Character: A curious robot exploring a new planet\n",
            "Setting: A vibrant, alien jungle on planet Xylar\n",
            "Plot Point: The robot finds an ancient artifact that hums with energy\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "In the heart of the alien jungle on planet Xylar, a small robot named Rizo was on its usual exploration mission. Rizo was a curious robot, always eager to discover new things and learn more about the universe. Its metallic legs clicked rhythmically as it moved through the dense vegetation, the bright lights of its sensors scanning the environment.\n",
            "\n",
            "The jungle on Xylar was unlike anything Rizo had ever seen. The trees were tall and twisted, with leaves that shimmered in the strange, purple light that filtered through the canopy. The air was thick with the sounds of unknown creatures and the scent of exotic flowers. Rizo felt alive, like it was truly part of this vibrant world.\n",
            "\n",
            "As Rizo explored deeper into the jungle, it stumbled upon something that hummed with energy. It was an ancient artifact, unlike anything Rizo had ever seen. It was made of a material that seemed to glow with an inner light, and it pulsed with a rhythm that matched the beat of Rizo's own heart.\n",
            "\n",
            "Rizo was fascinated by the artifact and began to study it closely. It soon realized that the artifact was more than just a piece of technology - it was a living thing, filled with energy that pulsed and flowed like the jungle around it.\n",
            "\n",
            "Rizo spent days studying the artifact, learning all it could about its inner workings. It discovered that the artifact was a source of power, a key to unlocking the secrets of the universe. And as it studied, it felt a connection to the artifact, as if it was part of something much larger than itself.\n",
            "\n",
            "In the end, Rizo knew that it had to share its discovery with the rest of the universe. It packed the artifact carefully into its cargo bay and set off on a journey to find its home planet. As it traveled through the stars, Rizo felt a sense of purpose, like it was on a mission to change the course of history.\n",
            "\n",
            "And when it finally arrived on its home planet, Rizo was greeted as a hero. The ancient artifact it had discovered was a treasure beyond measure, a key to unlocking the full potential of the universe. And Rizo, the curious robot who had explored the alien jungle on planet Xylar, had played a part in its discovery.\n",
            "\n",
            "--- Generating Story 2 (Mystery) ---\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_1 = story_chain.run(input_data_1)\n",
        "print(story_1)\n",
        "input_data_2 = { \"genre\": \"Mystery\", \"main_character_description\": \"A witty detective with a keen eye for detail\", \"setting\": \"A foggy night in 1940s London\", \"plot_point\": \"The detective finds a cryptic note at a crime scene\" }\n",
        "print(f\"\\n--- Generating Story 2 (Mystery) ---\")\n",
        "story_2 = story_chain.run(input_data_2)\n",
        "print(story_2)"
      ],
      "metadata": {
        "id": "GTvwpyxO_xIy",
        "outputId": "c66ba167-fa90-4378-f726-670187069111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GTvwpyxO_xIy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Science Fiction\n",
            "Main Character: A curious robot exploring a new planet\n",
            "Setting: A vibrant, alien jungle on planet Xylar\n",
            "Plot Point: The robot finds an ancient artifact that hums with energy\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "In the heart of the alien jungle on planet Xylar, a small robot named Rizo was on its usual exploration mission. Rizo was a curious robot, always eager to discover new things and learn more about the universe. Its metallic legs clicked rhythmically as it moved through the dense vegetation, the bright lights of its sensors scanning the environment.\n",
            "\n",
            "The jungle on Xylar was unlike anything Rizo had ever seen. The trees were tall and twisted, with leaves that shimmered in the strange, purple light that filtered through the canopy. The air was thick with the sounds of unknown creatures and the scent of exotic flowers. Rizo felt alive, like it was truly part of this vibrant world.\n",
            "\n",
            "As Rizo explored deeper into the jungle, it stumbled upon something that hummed with energy. It was an ancient artifact, unlike anything Rizo had ever seen. It was made of a material that seemed to glow with an inner light, and it pulsed with a rhythm that matched the beat of Rizo's own heart.\n",
            "\n",
            "Rizo was fascinated by the artifact and began to study it closely. It soon realized that the artifact was more than just a piece of technology - it was a living thing, filled with energy that pulsed and flowed like the jungle around it.\n",
            "\n",
            "Rizo spent days studying the artifact, learning all it could about its inner workings. It discovered that the artifact was a source of power, a key to unlocking the secrets of the universe. And as it studied, it felt a connection to the artifact, as if it was part of something much larger than itself.\n",
            "\n",
            "In the end, Rizo knew that it had to share its discovery with the rest of the universe. It packed the artifact carefully into its cargo bay and set off on a journey to find its home planet. As it traveled through the stars, Rizo felt a sense of purpose, like it was on a mission to change the course of history.\n",
            "\n",
            "And when it finally arrived on its home planet, Rizo was greeted as a hero. The ancient artifact it had discovered was a treasure beyond measure, a key to unlocking the full potential of the universe. And Rizo, the curious robot who had explored the alien jungle on planet Xylar, had played a part in its discovery.\n",
            "\n",
            "--- Generating Story 2 (Mystery) ---\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b49a31",
      "metadata": {
        "id": "73b49a31"
      },
      "source": [
        "# This can take a moment to run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_1 = story_chain.run(input_data_1)\n",
        "print(story_1)\n",
        "Example_2_input_data_2 = { \"genre\": \"Mystery\", \"main_character_description\": \"A witty detective with a keen eye for detail\", \"setting\": \"A foggy night in 1940s London\", \"plot_point\": \"The detective finds a cryptic note at a crime scene\" }\n",
        "\n",
        "print(f\"\\n--- Generating Story 2 (Mystery) ---\")\n",
        "\n",
        "story_2 = story_chain.run(Example_2_input_data_2)\n",
        "print(story_2)\n",
        "\n",
        "story_2 = story_chain.run(input_data_2)\n",
        "print(story_2)"
      ],
      "metadata": {
        "id": "87yvFyITl3DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23cf5e4-42b0-4f3a-e580-7dfcdcefc824"
      },
      "id": "87yvFyITl3DR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-1d9940fe94c4>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  story_1 = story_chain.run(input_data_1)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Science Fiction\n",
            "Main Character: A curious robot exploring a new planet\n",
            "Setting: A vibrant, alien jungle on planet Xylar\n",
            "Plot Point: The robot finds an ancient artifact that hums with energy\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "In the heart of the alien jungle on planet Xylar, a small robot named Rizo was on its usual exploration mission. Rizo was a curious robot, always eager to discover new things and learn more about the universe. Its metallic legs clicked rhythmically as it moved through the dense vegetation, the bright lights of its sensors scanning the environment.\n",
            "\n",
            "The jungle on Xylar was unlike anything Rizo had ever seen. The trees were tall and twisted, with leaves that shimmered in the strange, purple light that filtered through the canopy. The air was thick with the sounds of unknown creatures and the scent of exotic flowers. Rizo felt alive, like it was truly part of this vibrant world.\n",
            "\n",
            "As Rizo explored deeper into the jungle, it stumbled upon something that hummed with energy. It was an ancient artifact, unlike anything Rizo had ever seen. It was made of a material that seemed to glow with an inner light, and it pulsed with a rhythm that matched the beat of Rizo's own heart.\n",
            "\n",
            "Rizo was fascinated by the artifact and began to study it closely. It soon realized that the artifact was more than just a piece of technology - it was a living thing, filled with energy that pulsed and flowed like the jungle around it.\n",
            "\n",
            "Rizo spent days studying the artifact, learning all it could about its inner workings. It discovered that the artifact was a source of power, a key to unlocking the secrets of the universe. And as it studied, it felt a connection to the artifact, as if it was part of something much larger than itself.\n",
            "\n",
            "In the end, Rizo knew that it had to share its discovery with the rest of the universe. It packed the artifact carefully into its cargo bay and set off on a journey to find its home planet. As it traveled through the stars, Rizo felt a sense of purpose, like it was on a mission to change the course of history.\n",
            "\n",
            "And when it finally arrived on its home planet, Rizo was greeted as a hero. The ancient artifact it had discovered was a treasure beyond measure, a key to unlocking the full potential of the universe. And Rizo, the curious robot who had explored the alien jungle on planet Xylar, had played a part in its discovery.\n",
            "\n",
            "--- Generating Story 2 (Mystery) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4ca22f",
      "metadata": {
        "id": "2a4ca22f"
      },
      "source": [
        "### Explanation:\n",
        "* We create an `LLMChain` by providing our `llm` instance and the `story_prompt_template`.\n",
        "* The `.run()` method of the chain can be used to execute it. You can pass a dictionary of input variables or pass them as keyword arguments.\n",
        "* **Note:** Running the LLM can take some time, especially the first time or with longer outputs. The generation lines are commented out by default to prevent accidental long runs during initial notebook execution. You can uncomment them to see the stories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2be134",
      "metadata": {
        "id": "cf2be134"
      },
      "source": [
        "## Part 5: Building the CLI Tool with `argparse`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece0930b",
      "metadata": {
        "id": "ece0930b"
      },
      "source": [
        "### Explanation:\n",
        "Now, let's create a Python script that can be run from the command line. We'll use the `argparse` module to accept story elements as command-line arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512bbae7",
      "metadata": {
        "id": "512bbae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9d7309a3-17b6-4e11-c199-fbcbc4e2501f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 166)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m166\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "# The following code is intended to be saved as a .py file (e.g., story_cli.py)\n",
        "# For demonstration, we'll define the main function and argument parsing here.\n",
        "\n",
        "cli_script_content = \"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline  # Updated import\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def generate_story_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"LangChain Story Generator CLI\")\n",
        "    parser.add_argument(\"--genre\", type=str, required=True, help=\"Genre of the story (e.g., Fantasy, Sci-Fi)\")\n",
        "    parser.add_argument(\"--character\", type=str, required=True, help=\"Description of the main character\")\n",
        "    parser.add_argument(\"--setting\", type=str, required=True, help=\"Setting of the story\")\n",
        "    parser.add_argument(\"--plot\", type=str, required=True, help=\"A key plot point\")\n",
        "    parser.add_argument(\"--max_tokens\", type=int, default=512, help=\"Max new tokens for the story length\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Initializing LLM... This may take a moment.\")\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=args.max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "# The following code is intended to be saved as a .py file (e.g., story_cli.py)\n",
        "# For demonstration, we'll define the main function and argument parsing here.\n",
        "\n",
        "cli_script_content = \"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline  # Updated import\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def generate_story_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"LangChain Story Generator CLI\")\n",
        "    parser.add_argument(\"--genre\", type=str, required=True, help=\"Genre of the story (e.g., Fantasy, Sci-Fi)\")\n",
        "    parser.add_argument(\"--character\", type=str, required=True, help=\"Description of the main character\")\n",
        "    parser.add_argument(\"--setting\", type=str, required=True, help=\"Setting of the story\")\n",
        "    parser.add_argument(\"--plot\", type=str, required=True, help=\"A key plot point\")\n",
        "    parser.add_argument(\"--max_tokens\", type=int, default=512, help=\"Max new tokens for the story length\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Initializing LLM... This may take a moment.\")\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=args.max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        story_prompt_template_str = \"\"\"<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
        "Genre: {genre}\n",
        "Main Character: {main_character_description}\n",
        "Setting: {setting}\n",
        "Plot Point: {plot_point}\n",
        "\n",
        "Story: [/INST]\"\"\"\n",
        "\n",
        "        story_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"genre\", \"main_character_description\", \"setting\", \"plot_point\"],\n",
        "            template=story_prompt_template_str\n",
        "        )\n",
        "\n",
        "        story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
        "\n",
        "        print(f\"\\nGenerating story with the following elements:\")\n",
        "        print(f\"- Genre: {args.genre}\")\n",
        "        print(f\"- Character: {args.character}\")\n",
        "        print(f\"- Setting: {args.setting}\")\n",
        "        print(f\"- Plot: {args.plot}\")\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "        story_input = {\n",
        "            \"genre\": args.genre,\n",
        "            \"main_character_description\": args.character,\n",
        "            \"setting\": args.setting,\n",
        "            \"plot_point\": args.plot\n",
        "        }\n",
        "\n",
        "        story_output = story_chain.invoke(story_input)  # Updated to invoke\n",
        "        print(\"\\n--- Generated Story ---\")\n",
        "        if isinstance(story_output, dict) and 'text' in story_output:\n",
        "            print(story_output['text'])\n",
        "        else:\n",
        "            print(f\"Output format unexpected or 'text' key missing. Full output: {story_output}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_story_cli()\n",
        "\"\"\"\n",
        "Setting: {setting}\n",
        "Plot Point: {plot_point}\n",
        "\n",
        "Story: [/INST]\"\"\"\n",
        "\n",
        "        story_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"genre\", \"main_character_description\", \"setting\", \"plot_point\"],\n",
        "            template=story_prompt_template_str\n",
        "        )\n",
        "\n",
        "        story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
        "\n",
        "        print(f\"\\nGenerating story with the following elements:\")\n",
        "        print(f\"- Genre: {args.genre}\")\n",
        "        print(f\"- Character: {args.character}\")\n",
        "        print(f\"- Setting: {args.setting}\")\n",
        "        print(f\"- Plot: {args.plot}\")\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "        story_input = {\n",
        "            \"genre\": args.genre,\n",
        "            \"main_character_description\": args.character,\n",
        "            \"setting\": args.setting,\n",
        "            \"plot_point\": args.plot\n",
        "        }\n",
        "\n",
        "        story_output = story_chain.invoke(story_input)  # Updated to invoke\n",
        "        print(\"\\n--- Generated Story ---\")\n",
        "        if isinstance(story_output, dict) and 'text' in story_output:\n",
        "            print(story_output['text'])\n",
        "        else:\n",
        "            print(f\"Output format unexpected or 'text' key missing. Full output: {story_output}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_story_cli()\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ( your existing cli_script_content definition here )\n",
        "cli_script_content = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline  # Updated import\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def generate_story_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"LangChain Story Generator CLI\")\n",
        "    parser.add_argument(\"--genre\", type=str, required=True, help=\"Genre of the story (e.g., Fantasy, Sci-Fi)\")\n",
        "    parser.add_argument(\"--character\", type=str, required=True, help=\"Description of the main character\")\n",
        "    parser.add_argument(\"--setting\", type=str, required=True, help=\"Setting of the story\")\n",
        "    parser.add_argument(\"--plot\", type=str, required=True, help=\"A key plot point\")\n",
        "    parser.add_argument(\"--max_tokens\", type=int, default=512, help=\"Max new tokens for the story length\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Initializing LLM... This may take a moment.\")\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=args.max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        story_prompt_template_str = \\\"\\\"\\\"<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
        "Genre: {genre}\n",
        "Main Character: {main_character_description}\n",
        "Setting: {setting}\n",
        "Plot Point: {plot_point}\n",
        "\n",
        "Story: [/INST]\\\"\\\"\\\"\n",
        "\n",
        "        story_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"genre\", \"main_character_description\", \"setting\", \"plot_point\"],\n",
        "            template=story_prompt_template_str\n",
        "        )\n",
        "\n",
        "        story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
        "\n",
        "        print(f\"\\\\nGenerating story with the following elements:\")\n",
        "        print(f\"- Genre: {args.genre}\")\n",
        "        print(f\"- Character: {args.character}\")\n",
        "        print(f\"- Setting: {args.setting}\")\n",
        "        print(f\"- Plot: {args.plot}\")\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "        story_input = {\n",
        "            \"genre\": args.genre,\n",
        "            \"main_character_description\": args.character,\n",
        "            \"setting\": args.setting,\n",
        "            \"plot_point\": args.plot\n",
        "        }\n",
        "\n",
        "        story_output = story_chain.invoke(story_input)  # Updated to invoke\n",
        "        print(\"\\\\n--- Generated Story ---\")\n",
        "        if isinstance(story_output, dict) and 'text' in story_output:\n",
        "            print(story_output['text'])\n",
        "        else:\n",
        "            print(f\"Output format unexpected or 'text' key missing. Full output: {story_output}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_story_cli()\n",
        "\"\"\"\n",
        "\n",
        "def save_script(content: str, filename: str = \"story_cli.py\"):\n",
        "    \"\"\"\n",
        "    Saves the given content string to a Python script file.\n",
        "    \"\"\"\n",
        "    path = os.path.abspath(filename)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content.lstrip(\"\\n\"))  # strip leading newline for clean file start\n",
        "    print(f\" Script saved to {path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_script(cli_script_content)\n"
      ],
      "metadata": {
        "id": "z7yVz9bhDezT",
        "outputId": "9846a1ae-97ef-483c-cd72-073d4a1915c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "z7yVz9bhDezT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Script saved to /content/story_cli.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ( your existing cli_script_content definition here )\n",
        "cli_script_content = \"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline  # Updated import\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def generate_story_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"LangChain Story Generator CLI\")\n",
        "    parser.add_argument(\"--genre\", type=str, required=True, help=\"Genre of the story (e.g., Fantasy, Sci-Fi)\")\n",
        "    parser.add_argument(\"--character\", type=str, required=True, help=\"Description of the main character\")\n",
        "    parser.add_argument(\"--setting\", type=str, required=True, help=\"Setting of the story\")\n",
        "    parser.add_argument(\"--plot\", type=str, required=True, help=\"A key plot point\")\n",
        "    parser.add_argument(\"--max_tokens\", type=int, default=512, help=\"Max new tokens for the story length\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Initializing LLM... This may take a moment.\")\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=args.max_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        story_prompt_template_str = \\\"\\\"\\\"<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
        "Genre: {genre}\n",
        "Main Character: {main_character_description}\n",
        "Setting: {setting}\n",
        "Plot Point: {plot_point}\n",
        "\n",
        "Story: [/INST]\\\"\\\"\\\"\n",
        "\n",
        "        story_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"genre\", \"main_character_description\", \"setting\", \"plot_point\"],\n",
        "            template=story_prompt_template_str\n",
        "        )\n",
        "\n",
        "        story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
        "\n",
        "        print(f\"\\\\nGenerating story with the following elements:\")\n",
        "        print(f\"- Genre: {args.genre}\")\n",
        "        print(f\"- Character: {args.character}\")\n",
        "        print(f\"- Setting: {args.setting}\")\n",
        "        print(f\"- Plot: {args.plot}\")\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "        story_input = {\n",
        "            \"genre\": args.genre,\n",
        "            \"main_character_description\": args.character,\n",
        "            \"setting\": args.setting,\n",
        "            \"plot_point\": args.plot\n",
        "        }\n",
        "\n",
        "        story_output = story_chain.invoke(story_input)  # Updated to invoke\n",
        "        print(\"\\\\n--- Generated Story ---\")\n",
        "        if isinstance(story_output, dict) and 'text' in story_output:\n",
        "            print(story_output['text'])\n",
        "        else:\n",
        "            print(f\"Output format unexpected or 'text' key missing. Full output: {story_output}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_story_cli()\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "v4agQckdBx_S"
      },
      "id": "v4agQckdBx_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_1 = story_chain.run(input_data_1)\n",
        "print(story_1)\n",
        "input_data_2 = { \"genre\": \"Mystery\", \"main_character_description\": \"A witty detective with a keen eye for detail\", \"setting\": \"A foggy night in 1940s London\", \"plot_point\": \"The detective finds a cryptic note at a crime scene\" }\n",
        "print(f\"\\n--- Generating Story 2 (Mystery) ---\")\n",
        "story_2 = story_chain.run(input_data_2)\n",
        "print(story_2)"
      ],
      "metadata": {
        "id": "Ne13tHlrBCgq",
        "outputId": "0e5228ca-1242-437b-a10c-26bac2c0b6b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ne13tHlrBCgq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Science Fiction\n",
            "Main Character: A curious robot exploring a new planet\n",
            "Setting: A vibrant, alien jungle on planet Xylar\n",
            "Plot Point: The robot finds an ancient artifact that hums with energy\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "In the heart of the alien jungle on planet Xylar, a small robot named Rizo was on its usual exploration mission. Rizo was a curious robot, always eager to discover new things and learn more about the universe. Its metallic legs clicked rhythmically as it moved through the dense vegetation, the bright lights of its sensors scanning the environment.\n",
            "\n",
            "The jungle on Xylar was unlike anything Rizo had ever seen. The trees were tall and twisted, with leaves that shimmered in the strange, purple light that filtered through the canopy. The air was thick with the sounds of unknown creatures and the scent of exotic flowers. Rizo felt alive, like it was truly part of this vibrant world.\n",
            "\n",
            "As Rizo explored deeper into the jungle, it stumbled upon something that hummed with energy. It was an ancient artifact, unlike anything Rizo had ever seen. It was made of a material that seemed to glow with an inner light, and it pulsed with a rhythm that matched the beat of Rizo's own heart.\n",
            "\n",
            "Rizo was fascinated by the artifact and began to study it closely. It soon realized that the artifact was more than just a piece of technology - it was a living thing, filled with energy that pulsed and flowed like the jungle around it.\n",
            "\n",
            "Rizo spent days studying the artifact, learning all it could about its inner workings. It discovered that the artifact was a source of power, a key to unlocking the secrets of the universe. And as it studied, it felt a connection to the artifact, as if it was part of something much larger than itself.\n",
            "\n",
            "In the end, Rizo knew that it had to share its discovery with the rest of the universe. It packed the artifact carefully into its cargo bay and set off on a journey to find its home planet. As it traveled through the stars, Rizo felt a sense of purpose, like it was on a mission to change the course of history.\n",
            "\n",
            "And when it finally arrived on its home planet, Rizo was greeted as a hero. The ancient artifact it had discovered was a treasure beyond measure, a key to unlocking the full potential of the universe. And Rizo, the curious robot who had explored the alien jungle on planet Xylar, had played a part in its discovery.\n",
            "\n",
            "--- Generating Story 2 (Mystery) ---\n",
            "\n",
            "<s>[INST] You are a creative storyteller. Write a short story based on the following elements:\n",
            "Genre: Mystery\n",
            "Main Character: A witty detective with a keen eye for detail\n",
            "Setting: A foggy night in 1940s London\n",
            "Plot Point: The detective finds a cryptic note at a crime scene\n",
            "\n",
            "Story: [/INST]\n",
            "\n",
            "It was a foggy night in 1940s London, and the rain was pouring down in sheets. The streets were slick and treacherous, and the thick fog made it nearly impossible to see anything beyond a few feet. Despite the gloomy weather, Detective Arthur Wainwright was in high spirits. He had just solved a particularly puzzling case, and he was on his way home when he stumbled upon a new mystery.\n",
            "\n",
            "As he walked down a dark alley, he noticed a faint light flickering in the distance. Curiosity piqued, he followed the light and soon found himself at the scene of a crime. A man had been found dead on the ground, and there was a cryptic note next to him. The note was written in a code that Wainwright couldn't decipher at first glance.\n",
            "\n",
            "But as he examined the scene more closely, he noticed a few things that caught his eye. The victim's pockets were empty, and there were no signs of a struggle. The only clue was the note, which seemed to be written in a language that only Wainwright could understand.\n",
            "\n",
            "He spent hours poring over the note, trying to decipher its meaning. Finally, after what felt like an eternity, he realized that the note was written in a simple substitution cipher. With a little bit of trial and error, he was able to decode the message.\n",
            "\n",
            "The message read: \"Meet me at midnight near the Tower Bridge. Bring the key.\" Wainwright was puzzled. Who could have left this message, and what did it mean? He decided to investigate further.\n",
            "\n",
            "As he walked towards the Tower Bridge, he couldn't shake the feeling that he was being watched. Suddenly, he heard a faint sound coming from behind him. He turned around, but there was no one there. He continued on his way, determined to solve this mystery.\n",
            "\n",
            "When he arrived at the Tower Bridge, he saw a figure standing in the shadows. As he approached, the figure stepped forward and introduced himself as a fellow detective. They exchanged a few words, and Wainwright realized that this was the person who had left the cryptic note.\n",
            "\n",
            "Together, they went to the scene of the crime and began to investigate. They soon discovered that the victim had been a wealthy businessman who had recently received a threatening letter. The letter had contained a demand for money, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644dc136",
      "metadata": {
        "id": "644dc136"
      },
      "source": [
        "### How to Run the CLI Script (from your terminal):\n",
        "1. Save the code above into a file named `story_cli.py` (this cell does it for you).\n",
        "2. Open your terminal.\n",
        "3. Navigate to the directory where you saved `story_cli.py`.\n",
        "4. Run the script with arguments, for example:\n",
        "```bash\n",
        "python story_cli.py --genre \"Adventure\" --character \"A fearless explorer\" --setting \"A lost temple deep in the Amazon\" --plot \"The explorer triggers an ancient trap\"\n",
        "```\n",
        "Or, to test it within this notebook (if you have a terminal or can run shell commands):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b24655",
      "metadata": {
        "id": "f8b24655"
      },
      "outputs": [],
      "source": [
        "# !python /content/story_cli.py --genre \"Comedy\" --character \"A clumsy robot chef\" --setting \"A chaotic kitchen during dinner rush\" --plot \"The robot accidentally bakes its own instruction manual into a cake\" --max_tokens 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e575b1",
      "metadata": {
        "id": "94e575b1"
      },
      "source": [
        "### Explanation:\n",
        "* **`argparse.ArgumentParser`**: Sets up the argument parser.\n",
        "* **`add_argument`**: Defines the command-line arguments we expect (`--genre`, `--character`, etc.). `required=True` makes them mandatory.\n",
        "* **`parser.parse_args()`**: Parses the arguments provided when the script is run.\n",
        "* **LLM and Chain Initialization**: The script re-initializes the LLM and LangChain components. In a more advanced setup, you might serialize a pre-trained chain or have a more efficient way to load the model if the CLI is run frequently.\n",
        "* **`if __name__ == \"__main__\":`**: Ensures the `generate_story_cli()` function runs when the script is executed directly.\n",
        "* The script is written to `/home/ubuntu/story_cli.py`. You can then run it from a terminal. The example command shows how to execute it.\n",
        "* The `!python ...` line in the cell above is commented out but shows how you could try to run it from within a Jupyter environment that supports shell commands."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5070c761",
      "metadata": {
        "id": "5070c761"
      },
      "source": [
        "## Part 6: Conclusion and Further Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b0def8",
      "metadata": {
        "id": "98b0def8"
      },
      "source": [
        "### Explanation:\n",
        "Today, we've covered the basics of using LangChain with an open-source LLM to build a story generator and a CLI tool. This is just the tip of the iceberg!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef61f77",
      "metadata": {
        "id": "3ef61f77"
      },
      "source": [
        "### Further Ideas:\n",
        "* **More Complex Chains:** Explore `SequentialChain` or `RouterChain` for more sophisticated workflows.\n",
        "* **Memory:** Add memory to chains to allow for conversational interactions.\n",
        "* **Output Parsers:** Use LangChain's output parsers to structure the LLM's output (e.g., into JSON).\n",
        "* **Different LLMs:** Experiment with other quantized models or different model architectures.\n",
        "* **Error Handling:** Add more robust error handling to the CLI tool.\n",
        "* **Advanced CLI Features:** Use libraries like `Typer` or `Click` for more advanced CLI development.\n",
        "\n",
        "---\n",
        "End of Notebook. Remember to uncomment and run the LLM cells if you want to see the generated stories!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resources-references-cell-new"
      },
      "id": "resources-references-cell-new",
      "source": [
        "## Resources and References\n",
        "\n",
        "This notebook demonstrates using LangChain with Hugging Face models for text generation. Below are some helpful resources:\n",
        "\n",
        "*   **LangChain Python Documentation:** [https://python.langchain.com/](https://python.langchain.com/) - The official documentation for LangChain, covering concepts, integrations, and examples.\n",
        "*   **LangChainHuggingFace Integration:** For details on using HuggingFace models with LangChain: [https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/) (Note: check for the latest community or specific integration docs like `langchain-huggingface`).\n",
        "*   **Hugging Face Transformers:** [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index) - Documentation for the Transformers library, model hub, and pipelines.\n",
        "*   **Mistral AI & Mistral-7B-Instruct-v0.1:** [https://mistral.ai/](https://mistral.ai/) and model card [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
        "*   **TinyLlama Project:** [https://github.com/jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama) - For information on the TinyLlama models, which are excellent for resource-constrained environments.\n",
        "*   **Bitsandbytes for Quantization:** [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes) - Essential for running large models with reduced memory via techniques like 4-bit quantization.\n",
        "*   **PyTorch:** [https://pytorch.org/](https://pytorch.org/) - The deep learning framework used by Hugging Face Transformers.\n",
        "*   **Google Colab:** [https://colab.research.google.com/](https://colab.research.google.com/) - The environment this notebook is designed for, offering free access to GPUs like the T4.\n",
        "\n",
        "### Key Concepts Used\n",
        "*   **Prompt Engineering:** Crafting effective prompts (like the `story_prompt_template_str`) is crucial for guiding the LLM's output.\n",
        "*   **Quantization:** Techniques like 4-bit quantization (`load_in_4bit=True`) reduce model size and memory usage, enabling larger models on GPUs like the T4.\n",
        "*   **LLM Chains (`LLMChain`):** A fundamental LangChain concept for combining an LLM with a prompt template to perform a specific task.\n",
        "*   **Hugging Face Pipelines:** A high-level API from the Transformers library for easy inference with pre-trained models.\n",
        "\n",
        "### Further Exploration\n",
        "*   **LangChain Expression Language (LCEL):** For more advanced chain construction, explore LCEL for its composability and streaming capabilities.\n",
        "*   **Other Quantization Methods (e.g., GPTQ):** If you need even smaller models or different performance characteristics, investigate other quantization libraries like AutoGPTQ.\n",
        "*   **Alternative Open-Source LLMs:** Explore other models on the Hugging Face Hub suitable for T4 GPUs (e.g., Phi-2, other Mistral variants). Remember to check their specific prompt formats."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}