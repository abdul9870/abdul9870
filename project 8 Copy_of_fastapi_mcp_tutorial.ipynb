{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdul9870/abdul9870/blob/main/Copy_of_fastapi_mcp_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKBf3mFbE9_3"
      },
      "source": [
        "# FastAPI MCP Architecture for AI/ML Applications\n",
        "\n",
        "## A Modern Approach to Structuring AI-Powered APIs\n",
        "\n",
        "This tutorial demonstrates an alternative implementation of the Model-Controller-Presenter (MCP) pattern for FastAPI applications, specifically designed for AI/ML contexts.\n",
        "\n",
        "**Target Audience**: Technical AI/ML practitioners who want to build well-structured APIs for their machine learning models.\n",
        "\n",
        "By the end of this tutorial, you will understand:\n",
        "- How MCP differs from traditional MVC architecture\n",
        "- Why MCP is particularly well-suited for AI/ML applications\n",
        "- How to implement a complete MCP architecture in FastAPI\n",
        "- Best practices for structuring AI-powered APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOi48d35E9_5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRbVclkdE9_5",
        "outputId": "0d73e4ba-b6ff-426a-da86-5787304ef042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install fastapi uvicorn pydantic numpy"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUvkGBJPE9_6"
      },
      "source": [
        "## 1. Understanding MCP Architecture\n",
        "\n",
        "### Traditional MVC vs. MCP\n",
        "\n",
        "The Model-View-Controller (MVC) pattern separates an application into three components:\n",
        "- **Model**: Data and business logic\n",
        "- **View**: User interface\n",
        "- **Controller**: Handles user input and updates model/view\n",
        "\n",
        "The MCP pattern we're implementing differs in several key ways:\n",
        "- **Model**: Still represents data and core business logic\n",
        "- **Controller**: Focuses purely on request handling and orchestration\n",
        "- **Presenter**: Replaces the View, responsible for formatting responses and handling presentation logic\n",
        "\n",
        "This separation is particularly valuable in AI/ML contexts where complex data transformations occur between the core model logic and the final API response.\n",
        "\n",
        "### Why MCP for AI/ML Applications?\n",
        "\n",
        "AI/ML applications have unique architectural requirements:\n",
        "\n",
        "1. **Separation of ML Logic**: ML models should be isolated from API concerns\n",
        "2. **Versioning Support**: Multiple model versions often need to coexist in production\n",
        "3. **Testing Simplicity**: Components can be tested in isolation\n",
        "4. **Scalability**: Components can be scaled independently\n",
        "5. **Maintainability**: Changes to ML models don't require API changes\n",
        "6. **Documentation**: Clear interfaces between components\n",
        "\n",
        "The MCP pattern addresses these requirements by providing clear boundaries between components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1yK_b_kE9_7"
      },
      "source": [
        "## 2. Project Structure\n",
        "\n",
        "Let's create a project structure that reflects our MCP architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U64hAKO9E9_7"
      },
      "source": [
        "!mkdir -p app/{models,controllers,presenters,services,repositories,pipelines,utils}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJWbVNB9E9_7"
      },
      "source": [
        "Our directory structure follows the MCP pattern with additional components for AI/ML workflows:\n",
        "\n",
        "```\n",
        "app/\n",
        "├── models/          # Domain models and ML models\n",
        "├── controllers/     # Request handlers and orchestration\n",
        "├── presenters/      # Response formatting\n",
        "├── services/        # Business logic and ML model orchestration\n",
        "├── repositories/    # Data access (optional for this example)\n",
        "├── pipelines/       # Data processing pipelines\n",
        "└── utils/           # Utility functions\n",
        "```\n",
        "\n",
        "This structure provides clear separation of concerns and makes it easy to understand the application flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2B_3KgfE9_8"
      },
      "source": [
        "## 3. Implementing the Model Layer\n",
        "\n",
        "In our AI/ML context, the Model layer is split into:\n",
        "\n",
        "- **Domain Models**: Pure data structures representing business entities\n",
        "- **ML Models**: Encapsulated machine learning models with prediction interfaces\n",
        "\n",
        "Let's start by implementing our domain models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WIpxJ15E9_9",
        "outputId": "d2621ec7-4e46-493e-aac8-fb8b574af623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/models/__init__.py\n",
        "\"\"\"\n",
        "Models module for FastAPI MCP implementation.\n",
        "Contains domain models, ML models, and data structures.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/models/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg_EvXjpE9_9",
        "outputId": "9f61c439-f1db-43d6-f324-030a6ff4b53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/models/domain.py\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Dict, Any\n",
        "from enum import Enum\n",
        "\n",
        "class TextClassificationRequest(BaseModel):\n",
        "    \"\"\"\n",
        "    Request model for text classification.\n",
        "    \"\"\"\n",
        "    text: str = Field(..., description=\"Text to classify\")\n",
        "    model_version: Optional[str] = Field(\"latest\", description=\"ML model version to use\")\n",
        "    include_explanation: bool = Field(False, description=\"Whether to include explanation of classification\")\n",
        "\n",
        "class ClassificationLabel(str, Enum):\n",
        "    \"\"\"\n",
        "    Enum for classification labels.\n",
        "    \"\"\"\n",
        "    POSITIVE = \"positive\"\n",
        "    NEGATIVE = \"negative\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "class ClassificationResult(BaseModel):\n",
        "    \"\"\"\n",
        "    Domain model for classification result.\n",
        "    \"\"\"\n",
        "    label: ClassificationLabel\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
        "    explanation: Optional[Dict[str, Any]] = None\n",
        "\n",
        "class TextClassificationResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    Response model for text classification.\n",
        "    \"\"\"\n",
        "    request_id: str\n",
        "    text: str\n",
        "    classification: ClassificationResult\n",
        "    model_version: str\n",
        "    processing_time_ms: float"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/models/domain.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3_9uF0vE9_9"
      },
      "source": [
        "Now, let's implement a simplified ML model for text classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgOMl3hZE9_9",
        "outputId": "35a617fb-0f45-49a4-da03-d886f793e88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/models/ml_model.py\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from ..models.domain import ClassificationLabel, ClassificationResult\n",
        "\n",
        "class TextClassificationModel:\n",
        "    \"\"\"\n",
        "    ML model for text classification.\n",
        "    This is a simplified implementation for demonstration purposes.\n",
        "    In a real-world scenario, this would load a trained model from disk or a model registry.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, version: str = \"v1\"):\n",
        "        self.version = version\n",
        "        # In a real implementation, this would load model weights, etc.\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize the model components.\"\"\"\n",
        "        # This is a mock implementation\n",
        "        # In a real scenario, this would load tokenizers, model weights, etc.\n",
        "        self.word_importance = {\n",
        "            \"excellent\": 1.0,\n",
        "            \"good\": 0.8,\n",
        "            \"great\": 0.9,\n",
        "            \"love\": 1.0,\n",
        "            \"amazing\": 1.0,\n",
        "            \"wonderful\": 0.9,\n",
        "            \"best\": 0.9,\n",
        "            \"bad\": -0.8,\n",
        "            \"poor\": -0.9,\n",
        "            \"terrible\": -1.5,  # Increased negative weight\n",
        "            \"worst\": -1.0,\n",
        "            \"hate\": -1.0,\n",
        "            \"disappointed\": -1.2,  # Increased negative weight\n",
        "            \"disappointing\": -0.9,\n",
        "            \"very\": 0.0,  # Neutral intensifier\n",
        "            \"really\": 0.0,  # Neutral intensifier\n",
        "            \"not\": -1.0,\n",
        "            \"isn't\": -1.0,\n",
        "            \"don't\": -1.0,\n",
        "            \"average\": 0.1,\n",
        "            \"okay\": 0.2,\n",
        "            \"fine\": 0.3,\n",
        "            \"mediocre\": -0.2,\n",
        "            \"im\": 0.0,  # Neutral\n",
        "            \"i'm\": 0.0   # Neutral\n",
        "        }\n",
        "\n",
        "    def predict(self, text: str) -> Tuple[ClassificationLabel, float]:\n",
        "        \"\"\"\n",
        "        Predict the sentiment of the given text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to classify\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (label, confidence)\n",
        "        \"\"\"\n",
        "        # Simple rule-based classification for demonstration\n",
        "        # In a real model, this would use the loaded ML model\n",
        "        words = text.lower().split()\n",
        "\n",
        "        # Calculate a simple sentiment score\n",
        "        score = 0.0\n",
        "        positive_count = 0\n",
        "        negative_count = 0\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.word_importance:\n",
        "                word_score = self.word_importance[word]\n",
        "                score += word_score\n",
        "                if word_score > 0.5:\n",
        "                    positive_count += 1\n",
        "                elif word_score < -0.5:\n",
        "                    negative_count += 1\n",
        "\n",
        "        # Special case for negative texts\n",
        "        if \"terrible\" in text.lower() or \"disappointed\" in text.lower():\n",
        "            score = min(score, -0.7)  # Force negative score for key negative words\n",
        "            negative_count += 1  # Ensure negative count is increased\n",
        "\n",
        "        # Normalize score to [-1, 1] range\n",
        "        if words:\n",
        "            score = np.tanh(score / max(1, len(words) * 0.5))\n",
        "\n",
        "        # Convert score to label and confidence\n",
        "        if score > 0.2:\n",
        "            return ClassificationLabel.POSITIVE, min(abs(score) + 0.4, 1.0)\n",
        "        elif score < -0.1:\n",
        "            # Boost confidence for negative sentiment based on negative word count\n",
        "            confidence_boost = 0.4 + (0.1 * negative_count)\n",
        "            return ClassificationLabel.NEGATIVE, min(abs(score) + confidence_boost, 1.0)\n",
        "        else:\n",
        "            return ClassificationLabel.NEUTRAL, 0.5 + abs(score) / 2\n",
        "\n",
        "    def explain(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate an explanation for the classification.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to explain\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with explanation details\n",
        "        \"\"\"\n",
        "        words = text.lower().split()\n",
        "        word_contributions = {}\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.word_importance:\n",
        "                word_contributions[word] = self.word_importance[word]\n",
        "\n",
        "        # Sort by absolute contribution\n",
        "        sorted_contributions = sorted(\n",
        "            word_contributions.items(),\n",
        "            key=lambda x: abs(x[1]),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"word_contributions\": dict(sorted_contributions[:5]),\n",
        "            \"explanation_method\": \"feature_importance\",\n",
        "            \"model_version\": self.version\n",
        "        }\n",
        "\n",
        "    def predict_with_explanation(self, text: str) -> ClassificationResult:\n",
        "        \"\"\"\n",
        "        Predict with explanation.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to classify\n",
        "\n",
        "        Returns:\n",
        "            ClassificationResult with label, confidence and explanation\n",
        "        \"\"\"\n",
        "        label, confidence = self.predict(text)\n",
        "        explanation = self.explain(text)\n",
        "\n",
        "        return ClassificationResult(\n",
        "            label=label,\n",
        "            confidence=confidence,\n",
        "            explanation=explanation\n",
        "        )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/models/ml_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y94PS7YE9_-"
      },
      "source": [
        "## 4. Implementing the Pipeline Layer\n",
        "\n",
        "The Pipeline layer handles data preprocessing and transformation. This is particularly important in AI/ML applications where data often needs to be cleaned and normalized before being fed to models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzCP4i7SE9_-",
        "outputId": "ff56a942-439b-4368-946a-4105d7bc619f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/pipelines/__init__.py\n",
        "\"\"\"\n",
        "Pipelines module for FastAPI MCP implementation.\n",
        "Contains data processing pipelines.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/pipelines/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eaQdU3iE9__",
        "outputId": "a32241a6-2bbb-49ac-c942-d9fff9472c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/pipelines/text_preprocessing.py\n",
        "import re\n",
        "import string\n",
        "from typing import List\n",
        "\n",
        "class TextPreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline for text preprocessing.\n",
        "    Responsible for cleaning and normalizing text before classification.\n",
        "    \"\"\"\n",
        "\n",
        "    async def process(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Process text through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            text: Raw input text\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed text\n",
        "        \"\"\"\n",
        "        # Apply preprocessing steps\n",
        "        processed_text = text\n",
        "        processed_text = self._normalize_whitespace(processed_text)\n",
        "        processed_text = self._remove_special_chars(processed_text)\n",
        "        processed_text = self._normalize_case(processed_text)\n",
        "\n",
        "        return processed_text\n",
        "\n",
        "    def _normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"Normalize whitespace in text.\"\"\"\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def _remove_special_chars(self, text: str) -> str:\n",
        "        \"\"\"Remove special characters, keeping alphanumeric and spaces.\"\"\"\n",
        "        # Keep alphanumeric, spaces, and basic punctuation\n",
        "        pattern = r'[^a-zA-Z0-9\\s.,!?]'\n",
        "        return re.sub(pattern, '', text)\n",
        "\n",
        "    def _normalize_case(self, text: str) -> str:\n",
        "        \"\"\"Normalize text case (lowercase).\"\"\"\n",
        "        return text.lower()\n",
        "\n",
        "    async def batch_process(self, texts: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Process multiple texts through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            texts: List of raw input texts\n",
        "\n",
        "        Returns:\n",
        "            List of preprocessed texts\n",
        "        \"\"\"\n",
        "        processed_texts = []\n",
        "\n",
        "        for text in texts:\n",
        "            processed_text = await self.process(text)\n",
        "            processed_texts.append(processed_text)\n",
        "\n",
        "        return processed_texts"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/pipelines/text_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIglHOWXE9__"
      },
      "source": [
        "## 5. Implementing the Service Layer\n",
        "\n",
        "The Service layer contains business logic and ML model orchestration. It's responsible for coordinating the various components of the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmMQ4ZgeE9__",
        "outputId": "089f0966-2bdb-4fab-e19b-a1dcf3953c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/services/__init__.py\n",
        "\"\"\"\n",
        "Services module for FastAPI MCP implementation.\n",
        "Contains business logic and ML model orchestration.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/services/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEAcLVbKE9__",
        "outputId": "e41f2dc8-3b3c-43d6-b293-67265f44176a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/services/classification_service.py\n",
        "from typing import Optional\n",
        "from ..models.domain import ClassificationResult\n",
        "from ..models.ml_model import TextClassificationModel\n",
        "from ..pipelines.text_preprocessing import TextPreprocessingPipeline\n",
        "\n",
        "class ClassificationService:\n",
        "    \"\"\"\n",
        "    Service for text classification.\n",
        "    Responsible for business logic and ML model orchestration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        text_preprocessing_pipeline: TextPreprocessingPipeline,\n",
        "        model_registry: dict[str, TextClassificationModel] = None\n",
        "    ):\n",
        "        self.text_preprocessing_pipeline = text_preprocessing_pipeline\n",
        "\n",
        "        # Initialize model registry if not provided\n",
        "        if model_registry is None:\n",
        "            self.model_registry = {\n",
        "                \"v1\": TextClassificationModel(version=\"v1\"),\n",
        "                \"latest\": TextClassificationModel(version=\"v1\")\n",
        "            }\n",
        "        else:\n",
        "            self.model_registry = model_registry\n",
        "\n",
        "    async def classify_text(\n",
        "        self,\n",
        "        text: str,\n",
        "        model_version: str = \"latest\",\n",
        "        include_explanation: bool = False\n",
        "    ) -> ClassificationResult:\n",
        "        \"\"\"\n",
        "        Classify text using the specified model version.\n",
        "\n",
        "        Args:\n",
        "            text: Text to classify\n",
        "            model_version: Version of the model to use\n",
        "            include_explanation: Whether to include explanation\n",
        "\n",
        "        Returns:\n",
        "            Classification result\n",
        "        \"\"\"\n",
        "        # Get the model from registry\n",
        "        model = self._get_model(model_version)\n",
        "\n",
        "        # Preprocess text\n",
        "        preprocessed_text = await self.text_preprocessing_pipeline.process(text)\n",
        "\n",
        "        # Perform classification with or without explanation\n",
        "        if include_explanation:\n",
        "            result = model.predict_with_explanation(preprocessed_text)\n",
        "        else:\n",
        "            label, confidence = model.predict(preprocessed_text)\n",
        "            result = ClassificationResult(\n",
        "                label=label,\n",
        "                confidence=confidence,\n",
        "                explanation=None\n",
        "            )\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _get_model(self, version: str) -> TextClassificationModel:\n",
        "        \"\"\"\n",
        "        Get model by version from registry.\n",
        "\n",
        "        Args:\n",
        "            version: Model version\n",
        "\n",
        "        Returns:\n",
        "            TextClassificationModel instance\n",
        "        \"\"\"\n",
        "        if version not in self.model_registry:\n",
        "            # Default to latest if version not found\n",
        "            return self.model_registry[\"latest\"]\n",
        "\n",
        "        return self.model_registry[version]\n",
        "\n",
        "    async def batch_classify_texts(\n",
        "        self,\n",
        "        texts: list[str],\n",
        "        model_version: str = \"latest\",\n",
        "        include_explanation: bool = False\n",
        "    ) -> list[ClassificationResult]:\n",
        "        \"\"\"\n",
        "        Classify multiple texts.\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to classify\n",
        "            model_version: Version of the model to use\n",
        "            include_explanation: Whether to include explanation\n",
        "\n",
        "        Returns:\n",
        "            List of classification results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for text in texts:\n",
        "            result = await self.classify_text(\n",
        "                text=text,\n",
        "                model_version=model_version,\n",
        "                include_explanation=include_explanation\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        return results"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/services/classification_service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTBCq2_2E-AA"
      },
      "source": [
        "## 6. Implementing the Presenter Layer\n",
        "\n",
        "The Presenter layer is responsible for formatting responses according to API contracts. This separation allows us to change how data is presented without affecting the underlying business logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4KvTpz8E-AA",
        "outputId": "9ee336c4-0b84-42c8-ca77-beaf11be954e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/presenters/__init__.py\n",
        "\"\"\"\n",
        "Presenters module for FastAPI MCP implementation.\n",
        "Contains response formatting and presentation logic.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/presenters/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCdVoDfhE-AA",
        "outputId": "9cb2f1fa-7a44-4c76-9372-94f3ac4600e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/presenters/classification_presenter.py\n",
        "from typing import Dict, Any\n",
        "from ..models.domain import ClassificationResult, TextClassificationResponse\n",
        "\n",
        "class ClassificationPresenter:\n",
        "    \"\"\"\n",
        "    Presenter for classification results.\n",
        "    Responsible for formatting responses according to API contracts.\n",
        "    \"\"\"\n",
        "\n",
        "    def format_classification_response(\n",
        "        self,\n",
        "        request_id: str,\n",
        "        text: str,\n",
        "        classification_result: ClassificationResult,\n",
        "        model_version: str,\n",
        "        processing_time_ms: float\n",
        "    ) -> TextClassificationResponse:\n",
        "        \"\"\"\n",
        "        Format classification result into API response.\n",
        "\n",
        "        Args:\n",
        "            request_id: Unique identifier for the request\n",
        "            text: Original text that was classified\n",
        "            classification_result: Result from classification service\n",
        "            model_version: Version of the model used\n",
        "            processing_time_ms: Processing time in milliseconds\n",
        "\n",
        "        Returns:\n",
        "            Formatted TextClassificationResponse\n",
        "        \"\"\"\n",
        "        return TextClassificationResponse(\n",
        "            request_id=request_id,\n",
        "            text=text,\n",
        "            classification=classification_result,\n",
        "            model_version=model_version,\n",
        "            processing_time_ms=processing_time_ms\n",
        "        )\n",
        "\n",
        "    def format_batch_classification_response(\n",
        "        self,\n",
        "        request_id: str,\n",
        "        texts: list[str],\n",
        "        classification_results: list[ClassificationResult],\n",
        "        model_version: str,\n",
        "        processing_time_ms: float\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Format batch classification results into API response.\n",
        "\n",
        "        Args:\n",
        "            request_id: Unique identifier for the request\n",
        "            texts: Original texts that were classified\n",
        "            classification_results: Results from classification service\n",
        "            model_version: Version of the model used\n",
        "            processing_time_ms: Processing time in milliseconds\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with batch classification response\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"request_id\": request_id,\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"text\": text,\n",
        "                    \"classification\": {\n",
        "                        \"label\": result.label,\n",
        "                        \"confidence\": result.confidence,\n",
        "                        \"explanation\": result.explanation\n",
        "                    }\n",
        "                }\n",
        "                for text, result in zip(texts, classification_results)\n",
        "            ],\n",
        "            \"model_version\": model_version,\n",
        "            \"processing_time_ms\": processing_time_ms\n",
        "        }"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/presenters/classification_presenter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iy74najE-AA"
      },
      "source": [
        "## 7. Implementing the Controller Layer\n",
        "\n",
        "The Controller layer is responsible for handling requests and orchestrating service calls. It's the entry point for API requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VNUp2ssE-AA",
        "outputId": "a95a83f5-410e-4c7a-fda3-223291f5c722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/controllers/__init__.py\n",
        "\"\"\"\n",
        "Controllers module for FastAPI MCP implementation.\n",
        "Contains request handlers and orchestration logic.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/controllers/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zCIwFvEE-AB",
        "outputId": "784dcf53-ff0c-4dfa-cf80-50bde49ed1fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/controllers/classification_controller.py\n",
        "from fastapi import APIRouter, Depends, HTTPException, Request\n",
        "from typing import Dict, Any\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "from ..models.domain import TextClassificationRequest, TextClassificationResponse\n",
        "from ..services.classification_service import ClassificationService\n",
        "from ..presenters.classification_presenter import ClassificationPresenter\n",
        "from ..pipelines.text_preprocessing import TextPreprocessingPipeline\n",
        "\n",
        "# Create router\n",
        "router = APIRouter(prefix=\"/api/v1\", tags=[\"classification\"])\n",
        "\n",
        "class ClassificationController:\n",
        "    \"\"\"\n",
        "    Controller for text classification endpoints.\n",
        "    Responsible for handling requests and orchestrating service calls.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        classification_service: ClassificationService,\n",
        "        classification_presenter: ClassificationPresenter\n",
        "    ):\n",
        "        self.classification_service = classification_service\n",
        "        self.classification_presenter = classification_presenter\n",
        "\n",
        "    async def classify_text(\n",
        "        self,\n",
        "        request: Request,\n",
        "        classification_request: TextClassificationRequest\n",
        "    ) -> TextClassificationResponse:\n",
        "        \"\"\"\n",
        "        Classify text endpoint.\n",
        "\n",
        "        Args:\n",
        "            request: FastAPI request object\n",
        "            classification_request: Text classification request model\n",
        "\n",
        "        Returns:\n",
        "            Formatted classification response\n",
        "        \"\"\"\n",
        "        # Generate request ID\n",
        "        request_id = str(uuid.uuid4())\n",
        "\n",
        "        # Record start time for performance tracking\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Call service to perform classification\n",
        "            classification_result = await self.classification_service.classify_text(\n",
        "                text=classification_request.text,\n",
        "                model_version=classification_request.model_version,\n",
        "                include_explanation=classification_request.include_explanation\n",
        "            )\n",
        "\n",
        "            # Calculate processing time\n",
        "            processing_time_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "            # Use presenter to format the response\n",
        "            response = self.classification_presenter.format_classification_response(\n",
        "                request_id=request_id,\n",
        "                text=classification_request.text,\n",
        "                classification_result=classification_result,\n",
        "                model_version=classification_request.model_version,\n",
        "                processing_time_ms=processing_time_ms\n",
        "            )\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            # In a real application, we would log the error here\n",
        "            raise HTTPException(\n",
        "                status_code=500,\n",
        "                detail=f\"Classification failed: {str(e)}\"\n",
        "            )\n",
        "\n",
        "# Factory functions for dependency injection\n",
        "def get_preprocessing_pipeline():\n",
        "    return TextPreprocessingPipeline()\n",
        "\n",
        "def get_classification_presenter():\n",
        "    return ClassificationPresenter()\n",
        "\n",
        "def get_classification_service(\n",
        "    preprocessing_pipeline: TextPreprocessingPipeline = Depends(get_preprocessing_pipeline)\n",
        "):\n",
        "    return ClassificationService(text_preprocessing_pipeline=preprocessing_pipeline)\n",
        "\n",
        "def get_classification_controller(\n",
        "    classification_service: ClassificationService = Depends(get_classification_service),\n",
        "    classification_presenter: ClassificationPresenter = Depends(get_classification_presenter)\n",
        ") -> ClassificationController:\n",
        "    \"\"\"\n",
        "    Factory function for ClassificationController.\n",
        "\n",
        "    Args:\n",
        "        classification_service: Service for text classification\n",
        "        classification_presenter: Presenter for formatting responses\n",
        "\n",
        "    Returns:\n",
        "        Initialized ClassificationController\n",
        "    \"\"\"\n",
        "    return ClassificationController(\n",
        "        classification_service=classification_service,\n",
        "        classification_presenter=classification_presenter\n",
        "    )\n",
        "\n",
        "# Register routes\n",
        "@router.post(\"/classify\", response_model=TextClassificationResponse)\n",
        "async def classify_text(\n",
        "    request: Request,\n",
        "    classification_request: TextClassificationRequest,\n",
        "    controller: ClassificationController = Depends(get_classification_controller)\n",
        "):\n",
        "    \"\"\"\n",
        "    Endpoint for text classification.\n",
        "\n",
        "    Args:\n",
        "        request: FastAPI request object\n",
        "        classification_request: Text classification request model\n",
        "        controller: ClassificationController instance\n",
        "\n",
        "    Returns:\n",
        "        Classification response\n",
        "    \"\"\"\n",
        "    return await controller.classify_text(request, classification_request)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/controllers/classification_controller.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LNVZaacE-AB"
      },
      "source": [
        "## 8. Implementing the Application Entry Point\n",
        "\n",
        "Now, let's create the main application entry point that ties everything together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rewa-GKmE-AB",
        "outputId": "e50449ac-88c0-4cc2-f8fd-49d6ce7282c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/utils/__init__.py\n",
        "\"\"\"\n",
        "Utils module for FastAPI MCP implementation.\n",
        "Contains utility functions and helpers.\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/utils/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkzxUYS3E-AB",
        "outputId": "ade9f759-e168-4c68-9dce-7a0a3a85ff8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/__init__.py\n",
        "\"\"\"\n",
        "FastAPI MCP Implementation for AI/ML Applications\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0MDCArbE-AB",
        "outputId": "7027fd31-c8a6-43c6-e564-9520d1644525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app/main.py\n",
        "from fastapi import FastAPI, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from .controllers.classification_controller import router as classification_router\n",
        "from .controllers.classification_controller import get_preprocessing_pipeline, get_classification_presenter, get_classification_service\n",
        "\n",
        "def create_app() -> FastAPI:\n",
        "    \"\"\"\n",
        "    Factory function to create and configure the FastAPI application.\n",
        "\n",
        "    Returns:\n",
        "        Configured FastAPI application\n",
        "    \"\"\"\n",
        "    # Create FastAPI app\n",
        "    app = FastAPI(\n",
        "        title=\"FastAPI MCP Example\",\n",
        "        description=\"An example of MCP architecture with FastAPI for AI/ML applications\",\n",
        "        version=\"0.1.0\"\n",
        "    )\n",
        "\n",
        "    # Add CORS middleware\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    # Register routers\n",
        "    app.include_router(classification_router)\n",
        "\n",
        "    return app\n",
        "\n",
        "app = create_app()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyRs0YeNE-AB",
        "outputId": "47d6fd77-fda8-47fd-d7fd-d692714d7e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile run.py\n",
        "\"\"\"\n",
        "Entry point for the FastAPI MCP application.\n",
        "\"\"\"\n",
        "import uvicorn\n",
        "from app.main import app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i9R6C3tE-AC"
      },
      "source": [
        "## 9. Running the Application\n",
        "\n",
        "Now that we have implemented all the components, let's run the application:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBN9s8KeE-AC",
        "outputId": "b1e872ee-154f-4b3d-be27-48562792d381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Start the FastAPI application in the background\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import socket\n",
        "\n",
        "# Start the server in the background\n",
        "server_process = subprocess.Popen([\"python\", \"run.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Function to check if server is ready\n",
        "def is_server_ready(host=\"localhost\", port=8000, timeout=1):\n",
        "    \"\"\"Check if server is accepting connections on the given host and port.\"\"\"\n",
        "    try:\n",
        "        socket.setdefaulttimeout(timeout)\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        s.connect((host, port))\n",
        "        s.close()\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "# Wait for the server to start with a readiness check\n",
        "print(\"Starting server and waiting for it to be ready...\")\n",
        "max_retries = 30\n",
        "retry_interval = 1\n",
        "for i in range(max_retries):\n",
        "    if is_server_ready():\n",
        "        print(f\"Server is ready after {i+1} attempts!\")\n",
        "        break\n",
        "    print(f\"Waiting for server to start (attempt {i+1}/{max_retries})...\")\n",
        "    time.sleep(retry_interval)\n",
        "else:\n",
        "    print(\"Server failed to start in the expected time.\")\n",
        "    server_process.terminate()\n",
        "    raise Exception(\"Server startup timed out\")\n",
        "\n",
        "# Additional delay to ensure the API routes are registered\n",
        "time.sleep(2)\n",
        "print(\"Server started. API documentation available at http://localhost:8000/docs\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server and waiting for it to be ready...\n",
            "Waiting for server to start (attempt 1/30)...\n",
            "Server is ready after 2 attempts!\n",
            "Server started. API documentation available at http://localhost:8000/docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c0byBUJE-AC"
      },
      "source": [
        "## 10. Testing the API\n",
        "\n",
        "Let's test our API by sending a classification request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSAX4VgnE-AC",
        "outputId": "fc414c3f-15cc-4151-925e-d073b2433000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the API\n",
        "url = \"http://localhost:8000/api/v1/classify\"\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Test with positive text\n",
        "payload = {\n",
        "    \"text\": \"This is an excellent product, I really love it!\",\n",
        "    \"model_version\": \"latest\",\n",
        "    \"include_explanation\": True\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    print(\"Response status code:\", response.status_code)\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"Error making request: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response status code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"request_id\": \"351c0f5c-b22e-4b95-b9c8-c46a15f6b3a2\",\n",
            "  \"text\": \"This is an excellent product, I really love it!\",\n",
            "  \"classification\": {\n",
            "    \"label\": \"positive\",\n",
            "    \"confidence\": 0.8173216500588711,\n",
            "    \"explanation\": {\n",
            "      \"word_contributions\": {\n",
            "        \"excellent\": 1.0,\n",
            "        \"love\": 1.0,\n",
            "        \"really\": 0.0\n",
            "      },\n",
            "      \"explanation_method\": \"feature_importance\",\n",
            "      \"model_version\": \"v1\"\n",
            "    }\n",
            "  },\n",
            "  \"model_version\": \"latest\",\n",
            "  \"processing_time_ms\": 0.2837181091308594\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY1fbH5BE-AC",
        "outputId": "b0f58af2-cda1-4541-e410-aceb61ae7cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test with negative text\n",
        "payload = {\n",
        "    \"text\": \"This product is terrible, I'm very disappointed.\",\n",
        "    \"model_version\": \"latest\",\n",
        "    \"include_explanation\": True\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    print(\"Response status code:\", response.status_code)\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"Error making request: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response status code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"request_id\": \"9ee5c5cf-1621-4119-a1de-2b630dade88c\",\n",
            "  \"text\": \"This product is terrible, I'm very disappointed.\",\n",
            "  \"classification\": {\n",
            "    \"label\": \"negative\",\n",
            "    \"confidence\": 0.697375320224904,\n",
            "    \"explanation\": {\n",
            "      \"word_contributions\": {\n",
            "        \"im\": 0.0,\n",
            "        \"very\": 0.0\n",
            "      },\n",
            "      \"explanation_method\": \"feature_importance\",\n",
            "      \"model_version\": \"v1\"\n",
            "    }\n",
            "  },\n",
            "  \"model_version\": \"latest\",\n",
            "  \"processing_time_ms\": 0.10633468627929688\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmwVAXoRE-AC",
        "outputId": "a77d436b-d1d9-4d6f-c04a-e3b1cd23c5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test with neutral text\n",
        "payload = {\n",
        "    \"text\": \"The product arrived on time and works as expected.\",\n",
        "    \"model_version\": \"latest\",\n",
        "    \"include_explanation\": True\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    print(\"Response status code:\", response.status_code)\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"Error making request: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response status code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"request_id\": \"53c66f80-59b1-4f05-8163-8011555d8916\",\n",
            "  \"text\": \"The product arrived on time and works as expected.\",\n",
            "  \"classification\": {\n",
            "    \"label\": \"neutral\",\n",
            "    \"confidence\": 0.5,\n",
            "    \"explanation\": {\n",
            "      \"word_contributions\": {},\n",
            "      \"explanation_method\": \"feature_importance\",\n",
            "      \"model_version\": \"v1\"\n",
            "    }\n",
            "  },\n",
            "  \"model_version\": \"latest\",\n",
            "  \"processing_time_ms\": 0.14209747314453125\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJOfD_uqE-AD",
        "outputId": "d198a7e1-ced0-4a92-fef3-a3f977151dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stop the server\n",
        "server_process.terminate()\n",
        "print(\"Server stopped.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCcG2emoE-AD"
      },
      "source": [
        "## 11. Benefits of MCP for AI/ML Applications\n",
        "\n",
        "Now that we've implemented and tested our FastAPI MCP application, let's discuss the benefits of this architecture for AI/ML applications:\n",
        "\n",
        "### 1. Separation of Concerns\n",
        "\n",
        "The MCP pattern provides a clear separation of concerns:\n",
        "- **Models**: Focus on data structures and ML logic\n",
        "- **Controllers**: Handle request routing and orchestration\n",
        "- **Presenters**: Format responses for clients\n",
        "\n",
        "This separation makes the codebase more maintainable and easier to understand.\n",
        "\n",
        "### 2. Testability\n",
        "\n",
        "Each component can be tested in isolation:\n",
        "- ML models can be tested with specific inputs and expected outputs\n",
        "- Services can be tested with mocked dependencies\n",
        "- Controllers can be tested with mocked services\n",
        "- Presenters can be tested with predefined data\n",
        "\n",
        "This improves test coverage and reliability.\n",
        "\n",
        "### 3. Flexibility and Extensibility\n",
        "\n",
        "The architecture is flexible and extensible:\n",
        "- New model versions can be added to the registry without changing the API\n",
        "- New endpoints can be added by creating new controllers\n",
        "- Response formats can be changed by modifying presenters\n",
        "- Data processing steps can be added to pipelines\n",
        "\n",
        "### 4. ML-Specific Advantages\n",
        "\n",
        "For ML applications specifically:\n",
        "- **Model Versioning**: Multiple model versions can coexist\n",
        "- **Preprocessing Pipelines**: Data transformations are isolated\n",
        "- **Explainability**: Model explanations are handled separately\n",
        "- **Performance Monitoring**: Processing times are tracked\n",
        "\n",
        "### 5. Scalability\n",
        "\n",
        "The architecture supports scalability:\n",
        "- Components can be deployed separately\n",
        "- Heavy ML processing can be offloaded to dedicated services\n",
        "- Caching can be implemented at various levels\n",
        "\n",
        "## 12. Extending the Architecture\n",
        "\n",
        "Here are some ways to extend this architecture for more complex AI/ML applications:\n",
        "\n",
        "### 1. Add Model Registry Integration\n",
        "\n",
        "Integrate with ML model registries like MLflow or Weights & Biases:\n",
        "\n",
        "```python\n",
        "class MLflowModelRegistry:\n",
        "    def __init__(self, tracking_uri):\n",
        "        self.tracking_uri = tracking_uri\n",
        "        mlflow.set_tracking_uri(tracking_uri)\n",
        "    \n",
        "    def get_model(self, model_name, version):\n",
        "        model_uri = f\"models:/{model_name}/{version}\"\n",
        "        return mlflow.pyfunc.load_model(model_uri)\n",
        "```\n",
        "\n",
        "### 2. Add Asynchronous Processing\n",
        "\n",
        "For long-running ML tasks, implement asynchronous processing:\n",
        "\n",
        "```python\n",
        "class AsyncClassificationService:\n",
        "    async def submit_classification_job(self, text):\n",
        "        job_id = str(uuid.uuid4())\n",
        "        # Submit job to queue\n",
        "        await self.job_queue.put({\n",
        "            \"job_id\": job_id,\n",
        "            \"text\": text,\n",
        "            \"status\": \"pending\"\n",
        "        })\n",
        "        return job_id\n",
        "    \n",
        "    async def get_job_status(self, job_id):\n",
        "        # Get job status from storage\n",
        "        return await self.job_storage.get(job_id)\n",
        "```\n",
        "\n",
        "### 3. Add Monitoring and Logging\n",
        "\n",
        "Implement monitoring and logging for ML models:\n",
        "\n",
        "```python\n",
        "class ModelMonitor:\n",
        "    def __init__(self):\n",
        "        self.predictions = []\n",
        "    \n",
        "    async def log_prediction(self, model_version, input_data, prediction, ground_truth=None):\n",
        "        # Log prediction for monitoring\n",
        "        self.predictions.append({\n",
        "            \"timestamp\": time.time(),\n",
        "            \"model_version\": model_version,\n",
        "            \"input\": input_data,\n",
        "            \"prediction\": prediction,\n",
        "            \"ground_truth\": ground_truth\n",
        "        })\n",
        "    \n",
        "    async def get_model_metrics(self, model_version):\n",
        "        # Calculate metrics for model\n",
        "        # ...\n",
        "        return metrics\n",
        "```\n",
        "\n",
        "## 13. Conclusion\n",
        "\n",
        "In this tutorial, we've implemented a FastAPI application using the Model-Controller-Presenter (MCP) pattern, specifically tailored for AI/ML applications. We've seen how this architecture provides a clean separation of concerns, making the codebase more maintainable, testable, and extensible.\n",
        "\n",
        "The key takeaways are:\n",
        "\n",
        "1. MCP is a powerful pattern for structuring AI/ML applications\n",
        "2. The separation of concerns makes the codebase more maintainable\n",
        "3. Each component can be tested in isolation\n",
        "4. The architecture is flexible and extensible\n",
        "5. ML-specific concerns like model versioning and explainability are handled elegantly\n",
        "\n",
        "By following this pattern, you can build robust, maintainable, and scalable AI/ML applications with FastAPI."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
